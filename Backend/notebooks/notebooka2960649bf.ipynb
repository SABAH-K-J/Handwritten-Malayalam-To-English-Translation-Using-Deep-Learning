{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ff241",
   "metadata": {
    "papermill": {
     "duration": 0.002039,
     "end_time": "2026-01-20T18:33:30.744988",
     "exception": false,
     "start_time": "2026-01-20T18:33:30.742949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82163014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T18:33:30.749386Z",
     "iopub.status.busy": "2026-01-20T18:33:30.749159Z",
     "iopub.status.idle": "2026-01-20T18:33:41.139306Z",
     "shell.execute_reply": "2026-01-20T18:33:41.138410Z"
    },
    "papermill": {
     "duration": 10.394467,
     "end_time": "2026-01-20T18:33:41.140877",
     "exception": false,
     "start_time": "2026-01-20T18:33:30.746410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Using Device: cuda\n",
      "üî§ Building Charset...\n",
      "‚úÖ Vocab Size: 96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using Device: {DEVICE}\")\n",
    "\n",
    "# AUTO-DETECT DATASET PATH\n",
    "search = glob.glob(\"/kaggle/input/malayalam-cleaned-dataset/CleanedDataset/rec_gt_train.txt\", recursive=True)\n",
    "if not search: raise FileNotFoundError(\"‚ùå Dataset not found!\")\n",
    "DATA_ROOT = os.path.dirname(search[0])\n",
    "TRAIN_TXT = search[0]\n",
    "val_search = glob.glob(\"/kaggle/input/malayalam-cleaned-dataset/CleanedDataset/rec_gt_test.txt\", recursive=True)\n",
    "VAL_TXT = val_search[0] if val_search else TRAIN_TXT \n",
    "\n",
    "CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# OPTIMIZED HYPERPARAMETERS\n",
    "BATCH_SIZE = 256        # Saturation for T4 GPUs\n",
    "NUM_EPOCHS = 40         # OneCycleLR needs defined epochs\n",
    "MAX_LR = 0.003          # Aggressive start for OneCycle\n",
    "IMG_H = 32\n",
    "\n",
    "# ==========================================\n",
    "# 3. ROBUST AUGMENTATION\n",
    "# ==========================================\n",
    "class RealWorldAugment:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() > self.prob: return img\n",
    "        \n",
    "        # 1. Elastic / Geometric Distortions\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-2, 2)\n",
    "            shear = random.uniform(-10, 10) \n",
    "            img = img.transform(img.size, Image.AFFINE, (1, shear/100, 0, 0, 1, 0), resample=Image.BILINEAR).rotate(angle)\n",
    "        \n",
    "        # 2. Line Noise\n",
    "        img = img.convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        w, h = img.size\n",
    "        if random.random() > 0.6:\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                x = random.randint(0, w); y = random.randint(0, 3)\n",
    "                draw.line([(x, y), (x + random.randint(5, 20), y)], fill=(255,255,255), width=1)\n",
    "        \n",
    "        return img.convert(\"L\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATASET (RAM CACHED)\n",
    "# ==========================================\n",
    "def build_charset():\n",
    "    print(\"üî§ Building Charset...\")\n",
    "    unique_chars = set()\n",
    "    with open(TRAIN_TXT, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t') if '\\t' in line else line.strip().split(' ', 1)\n",
    "            if len(parts) >= 2: unique_chars.update(list(parts[1]))\n",
    "    chars = sorted(list(unique_chars))\n",
    "    itos = ['<BLANK>'] + chars\n",
    "    stoi = {c: i for i, c in enumerate(itos)}\n",
    "    print(f\"‚úÖ Vocab Size: {len(itos)}\")\n",
    "    return itos, stoi\n",
    "\n",
    "ITOS, STOI = build_charset()\n",
    "NUM_CLASSES = len(ITOS)\n",
    "\n",
    "class MalayalamDataset(Dataset):\n",
    "    def __init__(self, listfile, root_dir, img_h=32, augment=False):\n",
    "        self.samples = []\n",
    "        self.cached_images = []\n",
    "        self.img_h = img_h\n",
    "        self.augment = augment\n",
    "        self.augmentor = RealWorldAugment(prob=0.8)\n",
    "        self.tensor_aug = T.Compose([T.ColorJitter(brightness=0.4, contrast=0.4)])\n",
    "\n",
    "        temp_samples = []\n",
    "        with open(listfile, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t') if '\\t' in line else line.strip().split(' ', 1)\n",
    "                if len(parts) < 2: continue\n",
    "                rel_path = parts[0].strip().lstrip('./').lstrip('/')\n",
    "                temp_samples.append((os.path.join(root_dir, rel_path), parts[1]))\n",
    "\n",
    "        print(f\"üöÄ Loading {len(temp_samples)} images into RAM...\")\n",
    "        for path, text in tqdm(temp_samples):\n",
    "            if not os.path.exists(path): continue\n",
    "            try:\n",
    "                img = Image.open(path).convert('L')\n",
    "                w, h = img.size\n",
    "                new_w = max(1, int(w * (self.img_h / h)))\n",
    "                img = img.resize((new_w, self.img_h), Image.BILINEAR)\n",
    "                self.cached_images.append(np.array(img, dtype=np.uint8))\n",
    "                self.samples.append(text)\n",
    "            except: continue\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.cached_images[idx])\n",
    "        text = self.samples[idx]\n",
    "        \n",
    "        if self.augment: img = self.augmentor(img)\n",
    "        \n",
    "        img_arr = np.array(img).astype(np.float32) / 255.0\n",
    "        img_arr = 1.0 - img_arr\n",
    "        img_t = torch.from_numpy(img_arr).unsqueeze(0)\n",
    "        \n",
    "        if self.augment: img_t = self.tensor_aug(img_t)\n",
    "        \n",
    "        label = [STOI[c] for c in text if c in STOI]\n",
    "        return img_t, torch.tensor(label, dtype=torch.long), text\n",
    "\n",
    "def pad_batch(batch):\n",
    "    imgs, labels, texts = zip(*batch)\n",
    "    max_w = max([img.shape[2] for img in imgs])\n",
    "    padded_imgs = torch.zeros(len(imgs), 1, IMG_H, max_w)\n",
    "    for i, img in enumerate(imgs): padded_imgs[i, :, :, :img.shape[2]] = img\n",
    "    targets = torch.cat(labels)\n",
    "    target_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    return padded_imgs, targets, target_lengths, texts\n",
    "\n",
    "# ==========================================\n",
    "# 5. MODEL: HIGH-RES CUSTOM CRNN\n",
    "# ==========================================\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None: identity = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class CustomCRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCRNN, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # High-Res Configuration:\n",
    "        # We only downsample width in Layer 2. Layers 3 & 4 keep width.\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)     # H/2, W/2\n",
    "        self.layer3 = self._make_layer(256, 2, stride=(2,1)) # H/4, W/2\n",
    "        self.layer4 = self._make_layer(512, 2, stride=(2,1)) # H/8, W/2\n",
    "\n",
    "        self.last_conv = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=2, stride=(2,1), padding=0), # H/16, W/2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Regularized RNN Head\n",
    "        self.rnn = nn.Sequential(\n",
    "            # Dropout=0.5 here is the key to fixing the 91% plateau\n",
    "            nn.LSTM(512, 256, bidirectional=True, batch_first=True, num_layers=2, dropout=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResNetBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResNetBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = x.squeeze(2).permute(0, 2, 1) \n",
    "        x, _ = self.rnn[0](x) # LSTM\n",
    "        x = self.rnn[1](x)    # Linear\n",
    "        x = self.rnn[2](x)    # ELU\n",
    "        x = self.rnn[3](x)    # Dropout\n",
    "        x = self.rnn[4](x)    # Linear\n",
    "        return x \n",
    "\n",
    "# ==========================================\n",
    "# 6. TRAINING LOOP\n",
    "# ==========================================\n",
    "def decode(logits):\n",
    "    probs = logits.softmax(2).argmax(2).transpose(0, 1)\n",
    "    results = []\n",
    "    for seq in probs:\n",
    "        res = []\n",
    "        prev = 0\n",
    "        for idx in seq:\n",
    "            idx = idx.item()\n",
    "            if idx != 0 and idx != prev: res.append(ITOS[idx])\n",
    "            prev = idx\n",
    "        results.append(\"\".join(res))\n",
    "    return results\n",
    "\n",
    "def run_training():\n",
    "    model = CustomCRNN(NUM_CLASSES).to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Multi-GPU: {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "        MULTI_GPU = True\n",
    "    else: MULTI_GPU = False\n",
    "\n",
    "    print(\"\\nüì¶ Loading Data...\")\n",
    "    train_ds = MalayalamDataset(TRAIN_TXT, DATA_ROOT, img_h=IMG_H, augment=True)\n",
    "    val_ds = MalayalamDataset(VAL_TXT, DATA_ROOT, img_h=IMG_H, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, num_workers=4)\n",
    "\n",
    "    # AdamW + OneCycleLR = SOTA Convergence\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=MAX_LR, epochs=NUM_EPOCHS, \n",
    "        steps_per_epoch=len(train_loader), pct_start=0.3, div_factor=25\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    print(\"üöÄ Starting Optimized Custom Training...\")\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for imgs, targets, target_lens, texts in pbar:\n",
    "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(imgs) # [Batch, Time, Class]\n",
    "            logits_ctc = logits.transpose(0, 1) # [Time, Batch, Class]\n",
    "            \n",
    "            input_lens = torch.full(size=(imgs.size(0),), fill_value=logits.size(1), dtype=torch.long).to(DEVICE)\n",
    "            loss = criterion(logits_ctc.log_softmax(2), targets, input_lens, target_lens)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        val_correct = 0; count = 0; val_cer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, _, _, texts in val_loader:\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                logits = model(imgs)\n",
    "                preds = decode(logits.transpose(0, 1))\n",
    "                \n",
    "                for pred, true_text in zip(preds, texts):\n",
    "                    if pred == true_text: val_correct += 1\n",
    "                    dist = editdistance.eval(pred, true_text)\n",
    "                    val_cer_sum += dist / max(1, len(true_text))\n",
    "                    count += 1\n",
    "\n",
    "        acc = val_correct / count\n",
    "        cer = val_cer_sum / count\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\nüìä Epoch {epoch}: Acc: {acc*100:.2f}% | CER: {cer:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            state = model.module.state_dict() if MULTI_GPU else model.state_dict()\n",
    "            torch.save(state, os.path.join(CHECKPOINT_DIR, 'best_custom_model.pth'))\n",
    "            print(\"üî• New Best Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf56dc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T18:33:41.145342Z",
     "iopub.status.busy": "2026-01-20T18:33:41.145007Z",
     "iopub.status.idle": "2026-01-20T20:37:46.596044Z",
     "shell.execute_reply": "2026-01-20T20:37:46.595017Z"
    },
    "papermill": {
     "duration": 7446.132096,
     "end_time": "2026-01-20T20:37:47.274572",
     "exception": false,
     "start_time": "2026-01-20T18:33:41.142476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ôªÔ∏è Loading Best Model from: /kaggle/input/custom-ocr-model/pytorch/default/1/best_custom_model.pth\n",
      "‚úÖ Weights Loaded! Starting Fine-Tuning...\n",
      "üöÄ Loading 85270 images into RAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85270/85270 [12:09<00:00, 116.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading 19635 images into RAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19635/19635 [02:48<00:00, 116.61it/s]\n",
      "Squeeze Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:13<00:00,  1.12s/it, loss=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 1: Acc: 92.94% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:47<00:00,  1.22s/it, loss=0.0055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 2: Acc: 92.99% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:35<00:00,  1.19s/it, loss=0.0030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 3: Acc: 92.92% | CER: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:30<00:00,  1.17s/it, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 4: Acc: 92.96% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:30<00:00,  1.17s/it, loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 5: Acc: 92.96% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:30<00:00,  1.17s/it, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 6: Acc: 92.93% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:27<00:00,  1.16s/it, loss=0.0047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 7: Acc: 93.00% | CER: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:29<00:00,  1.17s/it, loss=0.0187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 8: Acc: 92.96% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:27<00:00,  1.16s/it, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 9: Acc: 92.97% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:41<00:00,  1.20s/it, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 10: Acc: 93.01% | CER: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:42<00:00,  1.20s/it, loss=0.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 11: Acc: 92.97% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:28<00:00,  1.16s/it, loss=0.0012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 12: Acc: 92.90% | CER: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:30<00:00,  1.17s/it, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 13: Acc: 92.98% | CER: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:30<00:00,  1.17s/it, loss=0.0017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 14: Acc: 92.96% | CER: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squeeze Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [06:26<00:00,  1.16s/it, loss=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Squeeze 15: Acc: 92.96% | CER: 0.0113\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ‚ö° FINE-TUNING \"SQUEEZE\" SCRIPT\n",
    "# ==========================================\n",
    "# HYPERPARAMETERS FOR FINE-TUNING\n",
    "FT_EPOCHS = 15\n",
    "FT_LR = 1e-5  # Very low, constant learning rate\n",
    "FT_CHECKPOINT = '/kaggle/input/custom-ocr-model/pytorch/default/1/best_custom_model.pth'\n",
    "FT_OUTPUT = '/kaggle/working/checkpoints/best_custom_model_finetuned.pth'\n",
    "\n",
    "def run_finetuning():\n",
    "    print(f\"‚ôªÔ∏è Loading Best Model from: {FT_CHECKPOINT}\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = CustomCRNN(NUM_CLASSES).to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        MULTI_GPU = True\n",
    "    else: MULTI_GPU = False\n",
    "        \n",
    "    # 2. Load Weights\n",
    "    # We need to handle DataParallel wrapping if it was saved that way\n",
    "    state_dict = torch.load(FT_CHECKPOINT)\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError:\n",
    "        # If model was saved without DataParallel but we are using it now (or vice versa)\n",
    "        # We might need to add/remove \"module.\" prefix\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k.replace(\"module.\", \"\") if \"module.\" in k else k\n",
    "            new_state_dict[name] = v\n",
    "        # Try loading into the unwrapped model first if using DataParallel\n",
    "        if MULTI_GPU:\n",
    "            model.module.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            model.load_state_dict(new_state_dict)\n",
    "            \n",
    "    print(\"‚úÖ Weights Loaded! Starting Fine-Tuning...\")\n",
    "\n",
    "    # 3. Data Loaders (Reuse existing ones)\n",
    "    train_ds = MalayalamDataset(TRAIN_TXT, DATA_ROOT, img_h=IMG_H, augment=True)\n",
    "    val_ds = MalayalamDataset(VAL_TXT, DATA_ROOT, img_h=IMG_H, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, num_workers=4)\n",
    "\n",
    "    # 4. Optimizer (Constant Low LR)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=FT_LR, weight_decay=1e-2)\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    best_acc = 0.9302 # Start tracking from your current best\n",
    "    \n",
    "    for epoch in range(1, FT_EPOCHS + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Squeeze Epoch {epoch}\")\n",
    "        \n",
    "        for imgs, targets, target_lens, texts in pbar:\n",
    "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(imgs)\n",
    "            logits_ctc = logits.transpose(0, 1)\n",
    "            input_lens = torch.full(size=(imgs.size(0),), fill_value=logits.size(1), dtype=torch.long).to(DEVICE)\n",
    "            \n",
    "            loss = criterion(logits_ctc.log_softmax(2), targets, input_lens, target_lens)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        val_correct = 0; count = 0; val_cer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, _, _, texts in val_loader:\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                logits = model(imgs)\n",
    "                preds = decode(logits.transpose(0, 1))\n",
    "                \n",
    "                for pred, true_text in zip(preds, texts):\n",
    "                    if pred == true_text: val_correct += 1\n",
    "                    dist = editdistance.eval(pred, true_text)\n",
    "                    val_cer_sum += dist / max(1, len(true_text))\n",
    "                    count += 1\n",
    "\n",
    "        acc = val_correct / count\n",
    "        cer = val_cer_sum / count\n",
    "        \n",
    "        print(f\"\\nüìä Squeeze {epoch}: Acc: {acc*100:.2f}% | CER: {cer:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            state = model.module.state_dict() if MULTI_GPU else model.state_dict()\n",
    "            torch.save(state, FT_OUTPUT)\n",
    "            print(f\"üî• Squeezed New Best: {acc*100:.2f}% (Saved!)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_finetuning()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9295236,
     "sourceId": 14553042,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 565427,
     "modelInstanceId": 552871,
     "sourceId": 726288,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7463.726506,
   "end_time": "2026-01-20T20:37:51.515896",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T18:33:27.789390",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
