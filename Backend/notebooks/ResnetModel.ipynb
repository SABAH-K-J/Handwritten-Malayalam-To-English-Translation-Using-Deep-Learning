{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeEGmbXjRoVq",
        "outputId": "c67cae7a-0695-4d3a-a340-3960032a6d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Mounted at /content/drive\n",
            "üìÇ Unzipping dataset...\n",
            "‚úÖ Data Ready.\n",
            "üî§ Generating Charset...\n",
            "‚úÖ Charset created with 95 characters.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Install missing library\n",
        "!pip install editdistance\n",
        "\n",
        "# 2. Mount Drive\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 3. Unzip Data (Adjust the path if your zip file is named differently)\n",
        "ZIP_PATH = '/content/drive/MyDrive/CleanedDataset.zip'\n",
        "\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    if not os.path.exists('/content/CleanedDataset'):\n",
        "        print(\"üìÇ Unzipping dataset...\")\n",
        "        !unzip -q {ZIP_PATH} -d /content/\n",
        "        print(\"‚úÖ Data Ready.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Zip file not found at {ZIP_PATH}. Please check the path!\")\n",
        "\n",
        "# 4. Generate Charset (The Dictionary)\n",
        "print(\"üî§ Generating Charset...\")\n",
        "train_file = '/content/CleanedDataset/rec_gt_train.txt'\n",
        "charset_file = '/content/CleanedDataset/charset.txt'\n",
        "\n",
        "unique_chars = set()\n",
        "if os.path.exists(train_file):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t') if '\\t' in line else line.strip().split(' ', 1)\n",
        "            if len(parts) >= 2:\n",
        "                for char in parts[1]:\n",
        "                    unique_chars.add(char)\n",
        "\n",
        "    with open(charset_file, 'w', encoding='utf-8') as f:\n",
        "        for char in sorted(list(unique_chars)):\n",
        "            f.write(char + '\\n')\n",
        "    print(f\"‚úÖ Charset created with {len(unique_chars)} characters.\")\n",
        "else:\n",
        "    print(\"‚ùå Error: Training file not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import editdistance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & SETUP\n",
        "# ==========================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Paths\n",
        "DATA_ROOT = '/content/CleanedDataset'\n",
        "TRAIN_TXT = os.path.join(DATA_ROOT, 'rec_gt_train.txt')\n",
        "VAL_TXT   = os.path.join(DATA_ROOT, 'rec_gt_test.txt')\n",
        "CHARSET_FILE = os.path.join(DATA_ROOT, 'charset.txt')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/pytorch_ocr_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "IMG_H = 32\n",
        "\n",
        "# Load Charset\n",
        "if not os.path.exists(CHARSET_FILE):\n",
        "    raise FileNotFoundError(\"Run the Charset Generation step first!\")\n",
        "\n",
        "with open(CHARSET_FILE, 'r', encoding='utf-8') as f:\n",
        "    chars = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "itos = ['<BLANK>'] + chars\n",
        "stoi = {c: i for i, c in enumerate(itos)}\n",
        "NUM_CLASSES = len(itos)\n",
        "print(f\"Vocab size: {NUM_CLASSES}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATASET CLASS\n",
        "# ==========================================\n",
        "class MalayalamDataset(Dataset):\n",
        "    def __init__(self, listfile, root_dir, img_h=32, augment=False):\n",
        "        self.samples = []\n",
        "        self.root_dir = root_dir\n",
        "        self.img_h = img_h\n",
        "        self.augment = augment\n",
        "        self.transform = T.Compose([\n",
        "            T.RandomAffine(degrees=2, translate=(0.02, 0.02), shear=2),\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "        ])\n",
        "\n",
        "        with open(listfile, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                if '\\t' in line: parts = line.split('\\t')\n",
        "                else: parts = line.split(' ', 1)\n",
        "                if len(parts) < 2: continue\n",
        "\n",
        "                img_path = parts[0].replace(\"./\", \"\")\n",
        "                self.samples.append((os.path.join(root_dir, img_path), parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, text = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(path).convert('L')\n",
        "        except:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        w, h = img.size\n",
        "        new_w = max(1, int(w * (self.img_h / h)))\n",
        "        img = img.resize((new_w, self.img_h), Image.BILINEAR)\n",
        "        img_arr = np.array(img).astype(np.float32) / 255.0\n",
        "        img_arr = 1.0 - img_arr\n",
        "        img_t = torch.from_numpy(img_arr).unsqueeze(0)\n",
        "\n",
        "        if self.augment:\n",
        "            img_t = self.transform(img_t)\n",
        "\n",
        "        label = [stoi[c] for c in text if c in stoi]\n",
        "        return img_t, torch.tensor(label, dtype=torch.long), text\n",
        "\n",
        "def pad_batch(batch):\n",
        "    imgs, labels, texts = zip(*batch)\n",
        "    widths = [img.shape[2] for img in imgs]\n",
        "    max_w = max(widths)\n",
        "    padded_imgs = torch.zeros(len(imgs), 1, IMG_H, max_w)\n",
        "    for i, img in enumerate(imgs):\n",
        "        w = img.shape[2]\n",
        "        padded_imgs[i, :, :, :w] = img\n",
        "\n",
        "    target_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
        "    targets = torch.cat(labels)\n",
        "    return padded_imgs, targets, target_lengths, texts\n",
        "\n",
        "# ==========================================\n",
        "# 3. RESNET MODEL ARCHITECTURE\n",
        "# ==========================================\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetCRNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetCRNN, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=(2,1))\n",
        "        self.layer4 = self._make_layer(512, 2, stride=(2,1))\n",
        "\n",
        "        self.last_conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=2, stride=(2,1), padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.rnn = nn.Sequential(\n",
        "            nn.LSTM(512, 256, bidirectional=True, batch_first=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(ResNetBlock(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResNetBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.last_conv(x)\n",
        "        x = x.squeeze(2).permute(0, 2, 1)\n",
        "        x, _ = self.rnn[0](x)\n",
        "        x = self.rnn[1](x)\n",
        "        x = self.rnn[2](x)\n",
        "        x = self.rnn[3](x)\n",
        "        return x.transpose(0, 1)\n",
        "\n",
        "def decode(logits):\n",
        "    probs = logits.softmax(2).argmax(2).transpose(0, 1)\n",
        "    results = []\n",
        "    for seq in probs:\n",
        "        res = []\n",
        "        prev = 0\n",
        "        for idx in seq:\n",
        "            idx = idx.item()\n",
        "            if idx != 0 and idx != prev:\n",
        "                res.append(itos[idx])\n",
        "            prev = idx\n",
        "        results.append(\"\".join(res))\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAINING LOOP (CORRECTED)\n",
        "# ==========================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rHcpCK_R-GI",
        "outputId": "5e2a209b-a36c-41d7-bdea-0d647fabff1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Vocab size: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_resnet_training():\n",
        "    train_ds = MalayalamDataset(TRAIN_TXT, DATA_ROOT, img_h=IMG_H, augment=True)\n",
        "    val_ds = MalayalamDataset(VAL_TXT, DATA_ROOT, img_h=IMG_H)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch)\n",
        "\n",
        "    print(f\"Loaded {len(train_ds)} Training, {len(val_ds)} Validation\")\n",
        "\n",
        "    model = ResNetCRNN(NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # FIX: Removed 'verbose=True' to fix the TypeError\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    print(\"üöÄ Starting ResNet-CRNN Training...\")\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_accum = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "        for imgs, targets, target_lens, texts in pbar:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            logits = model(imgs)\n",
        "            input_lens = torch.full(size=(imgs.size(0),), fill_value=logits.size(0), dtype=torch.long).to(DEVICE)\n",
        "            loss = criterion(logits.log_softmax(2), targets, input_lens, target_lens)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "            loss_accum += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        count = 0\n",
        "        val_cer_sum = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, _, _, texts in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                logits = model(imgs)\n",
        "                preds = decode(logits)\n",
        "                for pred, true_text in zip(preds, texts):\n",
        "                    if pred == true_text: val_correct += 1\n",
        "                    dist = editdistance.eval(pred, true_text)\n",
        "                    val_cer_sum += dist / max(1, len(true_text))\n",
        "                    count += 1\n",
        "\n",
        "        accuracy = val_correct / count\n",
        "        avg_cer = val_cer_sum / count\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(accuracy)\n",
        "\n",
        "        # We manually print the learning rate since verbose=True is removed\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"\\nüìä Epoch {epoch}: Val Acc: {accuracy*100:.2f}% | Val CER: {avg_cer:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_resnet_model.pth'))\n",
        "            print(f\"üî• Best Model Saved!\")\n",
        "\n",
        "# Execute\n",
        "run_resnet_training()"
      ],
      "metadata": {
        "id": "_LQVslvZxbHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# RESUME TRAINING SCRIPT\n",
        "# ==========================================\n",
        "def resume_training():\n",
        "    # 1. Setup Data & Model\n",
        "    train_ds = MalayalamDataset(TRAIN_TXT, DATA_ROOT, img_h=IMG_H, augment=True)\n",
        "    val_ds = MalayalamDataset(VAL_TXT, DATA_ROOT, img_h=IMG_H)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch)\n",
        "\n",
        "    print(f\"Loaded {len(train_ds)} Training, {len(val_ds)} Validation\")\n",
        "\n",
        "    model = ResNetCRNN(NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "    # 2. LOAD WEIGHTS FROM DRIVE\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_resnet_model.pth')\n",
        "    start_epoch = 1\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"üîÑ Found checkpoint at: {checkpoint_path}\")\n",
        "        print(\"üì• Loading weights to resume training...\")\n",
        "        model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "\n",
        "        # We assume we are restarting around epoch 9\n",
        "        start_epoch = 9\n",
        "        # Set baseline high so we don't overwrite the good model with a bad first epoch\n",
        "        best_accuracy = 0.8493\n",
        "        print(f\"‚úÖ Model loaded! Resuming from approx Epoch {start_epoch}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No checkpoint found! Starting from scratch.\")\n",
        "\n",
        "    # 3. Setup Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # Re-initialize scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    print(\"üöÄ Resuming ResNet-CRNN Training...\")\n",
        "\n",
        "    # Continue for remaining epochs (e.g., 50 - 9 = 41 more epochs)\n",
        "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_accum = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "        for imgs, targets, target_lens, texts in pbar:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            logits = model(imgs)\n",
        "            input_lens = torch.full(size=(imgs.size(0),), fill_value=logits.size(0), dtype=torch.long).to(DEVICE)\n",
        "            loss = criterion(logits.log_softmax(2), targets, input_lens, target_lens)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "            loss_accum += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        count = 0\n",
        "        val_cer_sum = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, _, _, texts in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                logits = model(imgs)\n",
        "                preds = decode(logits)\n",
        "                for pred, true_text in zip(preds, texts):\n",
        "                    if pred == true_text: val_correct += 1\n",
        "                    dist = editdistance.eval(pred, true_text)\n",
        "                    val_cer_sum += dist / max(1, len(true_text))\n",
        "                    count += 1\n",
        "\n",
        "        accuracy = val_correct / count\n",
        "        avg_cer = val_cer_sum / count\n",
        "\n",
        "        scheduler.step(accuracy)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"\\nüìä Epoch {epoch}: Val Acc: {accuracy*100:.2f}% | Val CER: {avg_cer:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Only save if we beat the previous best (0.8493)\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_resnet_model.pth'))\n",
        "            print(f\"üî• New Best Model Saved! ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Execute\n",
        "resume_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFdHzQLOwWMb",
        "outputId": "5b7ca995-2853-4433-9068-23846d715fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 85270 Training, 19635 Validation\n",
            "üîÑ Found checkpoint at: /content/drive/MyDrive/pytorch_ocr_checkpoints/best_resnet_model.pth\n",
            "üì• Loading weights to resume training...\n",
            "‚úÖ Model loaded! Resuming from approx Epoch 9\n",
            "üöÄ Resuming ResNet-CRNN Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:57<00:00,  2.23it/s, loss=0.0237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 9: Val Acc: 85.83% | Val CER: 0.0225 | LR: 0.001000\n",
            "üî• New Best Model Saved! (85.83%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.23it/s, loss=0.0372]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 10: Val Acc: 85.83% | Val CER: 0.0229 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.23it/s, loss=0.0611]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 11: Val Acc: 84.36% | Val CER: 0.0249 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:58<00:00,  2.23it/s, loss=0.0498]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 12: Val Acc: 86.03% | Val CER: 0.0226 | LR: 0.001000\n",
            "üî• New Best Model Saved! (86.03%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:57<00:00,  2.23it/s, loss=0.0192]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 13: Val Acc: 86.00% | Val CER: 0.0225 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:59<00:00,  2.22it/s, loss=0.0298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 14: Val Acc: 87.24% | Val CER: 0.0200 | LR: 0.001000\n",
            "üî• New Best Model Saved! (87.24%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:57<00:00,  2.23it/s, loss=0.0236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 15: Val Acc: 86.12% | Val CER: 0.0219 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:58<00:00,  2.23it/s, loss=0.0394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 16: Val Acc: 87.47% | Val CER: 0.0197 | LR: 0.001000\n",
            "üî• New Best Model Saved! (87.47%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.24it/s, loss=0.0881]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 17: Val Acc: 88.08% | Val CER: 0.0194 | LR: 0.001000\n",
            "üî• New Best Model Saved! (88.08%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.23it/s, loss=0.0053]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 18: Val Acc: 87.79% | Val CER: 0.0196 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.24it/s, loss=0.0798]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 19: Val Acc: 89.09% | Val CER: 0.0170 | LR: 0.001000\n",
            "üî• New Best Model Saved! (89.09%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:54<00:00,  2.24it/s, loss=0.0465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 20: Val Acc: 87.40% | Val CER: 0.0197 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.24it/s, loss=0.0485]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 21: Val Acc: 89.08% | Val CER: 0.0174 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:55<00:00,  2.24it/s, loss=0.0117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 22: Val Acc: 88.20% | Val CER: 0.0187 | LR: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:55<00:00,  2.24it/s, loss=0.0226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 23: Val Acc: 88.36% | Val CER: 0.0183 | LR: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.23it/s, loss=0.0019]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 24: Val Acc: 90.20% | Val CER: 0.0155 | LR: 0.000500\n",
            "üî• New Best Model Saved! (90.20%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:57<00:00,  2.23it/s, loss=0.0008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 25: Val Acc: 89.48% | Val CER: 0.0168 | LR: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [09:56<00:00,  2.23it/s, loss=0.0008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 26: Val Acc: 90.04% | Val CER: 0.0154 | LR: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1094/1333 [08:10<01:44,  2.28it/s, loss=0.0065]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7yJMsJNC-GPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import editdistance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "DATA_ROOT = '/content/CleanedDataset'\n",
        "TRAIN_TXT = os.path.join(DATA_ROOT, 'rec_gt_train.txt')\n",
        "VAL_TXT   = os.path.join(DATA_ROOT, 'rec_gt_test.txt')\n",
        "CHARSET_FILE = os.path.join(DATA_ROOT, 'charset.txt')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/pytorch_ocr_checkpoints'\n",
        "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'resnet_best.pth') # Loading the ResNet one\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 40\n",
        "IMG_H = 32\n",
        "LEARNING_RATE = 0.0003\n",
        "\n",
        "print(f\"üöÄ Resuming on Device: {DEVICE}\")\n",
        "\n",
        "# Load Charset\n",
        "if os.path.exists(CHARSET_FILE):\n",
        "    with open(CHARSET_FILE, 'r', encoding='utf-8') as f:\n",
        "        chars = [line.strip() for line in f if line.strip()]\n",
        "    itos = ['<BLANK>'] + chars\n",
        "    stoi = {c: i for i, c in enumerate(itos)}\n",
        "    NUM_CLASSES = len(itos)\n",
        "else:\n",
        "    print(\"‚ùå Error: Charset missing. Run Step 1!\")\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "# ==========================================\n",
        "# 2. RE-DEFINE CLASSES\n",
        "# ==========================================\n",
        "class MalayalamDataset(Dataset):\n",
        "    def __init__(self, listfile, root_dir, img_h=32, augment=False):\n",
        "        self.samples = []\n",
        "        self.root_dir = root_dir\n",
        "        self.img_h = img_h\n",
        "        self.augment = augment\n",
        "\n",
        "        # Harder Augmentation (Same as before)\n",
        "        self.transform = T.Compose([\n",
        "            T.RandomAffine(degrees=3, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=10),\n",
        "            T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n",
        "            T.ColorJitter(brightness=0.4, contrast=0.4)\n",
        "        ])\n",
        "\n",
        "        if os.path.exists(listfile):\n",
        "            with open(listfile, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    if '\\t' in line: parts = line.split('\\t')\n",
        "                    else: parts = line.split(' ', 1)\n",
        "                    if len(parts) < 2: continue\n",
        "                    path = parts[0].replace(\"./\", \"\")\n",
        "                    self.samples.append((os.path.join(root_dir, path), parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, text = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(path).convert('L')\n",
        "        except:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        w, h = img.size\n",
        "        new_w = max(1, int(w * (self.img_h / h)))\n",
        "        img = img.resize((new_w, self.img_h), Image.BILINEAR)\n",
        "        img_arr = np.array(img).astype(np.float32) / 255.0\n",
        "        img_arr = 1.0 - img_arr\n",
        "        img_t = torch.from_numpy(img_arr).unsqueeze(0)\n",
        "\n",
        "        if self.augment: img_t = self.transform(img_t)\n",
        "        label = [stoi.get(c, 0) for c in text]\n",
        "        return img_t, torch.tensor(label, dtype=torch.long), text\n",
        "\n",
        "def pad_batch(batch):\n",
        "    imgs, labels, texts = zip(*batch)\n",
        "    widths = [img.shape[2] for img in imgs]\n",
        "    max_w = max(widths)\n",
        "    padded_imgs = torch.zeros(len(imgs), 1, IMG_H, max_w)\n",
        "    for i, img in enumerate(imgs):\n",
        "        w = img.shape[2]\n",
        "        padded_imgs[i, :, :, :w] = img\n",
        "    target_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
        "    targets = torch.cat(labels)\n",
        "    return padded_imgs, targets, target_lengths, texts\n",
        "\n",
        "class CRNN_ResNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.rnn = nn.Sequential(\n",
        "            nn.LSTM(512, 256, bidirectional=True, batch_first=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.cnn(x).squeeze(2).permute(0, 2, 1)\n",
        "        output = self.rnn[0](features)[0]\n",
        "        return self.rnn[3](self.rnn[2](self.rnn[1](output))).transpose(0, 1)\n",
        "\n",
        "def decode(logits):\n",
        "    probs = logits.softmax(2).argmax(2).transpose(0, 1)\n",
        "    results = []\n",
        "    for seq in probs:\n",
        "        res = []\n",
        "        prev = 0\n",
        "        for idx in seq:\n",
        "            idx = idx.item()\n",
        "            if idx != 0 and idx != prev: res.append(itos[idx])\n",
        "            prev = idx\n",
        "        results.append(\"\".join(res))\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# 3. RESUME LOGIC\n",
        "# ==========================================\n",
        "def resume_training():\n",
        "    print(\"‚è≥ Loading Data...\")\n",
        "    train_ds = MalayalamDataset(TRAIN_TXT, DATA_ROOT, img_h=IMG_H, augment=True)\n",
        "    val_ds = MalayalamDataset(VAL_TXT, DATA_ROOT, img_h=IMG_H)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=pad_batch)\n",
        "\n",
        "    model = CRNN_ResNet(NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    # CHECKPOINT LOADING\n",
        "    best_accuracy = 0.0\n",
        "    start_epoch = 1\n",
        "\n",
        "    if os.path.exists(BEST_MODEL_PATH):\n",
        "        print(f\"üîÑ Found checkpoint: {BEST_MODEL_PATH}\")\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
        "            print(\"‚úÖ Weights Loaded! Continuing training...\")\n",
        "            # Set baseline to beat (e.g. 50%) so we don't save a bad model immediately\n",
        "            best_accuracy = 0.50\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading weights: {e}\")\n",
        "            print(\"Starting fresh...\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No checkpoint found. Starting fresh.\")\n",
        "\n",
        "    print(f\"üöÄ Training Loop Restarted...\")\n",
        "\n",
        "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_accum = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Resume Epoch {epoch}\")\n",
        "\n",
        "        for imgs, targets, target_lens, texts in pbar:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            logits = model(imgs)\n",
        "            input_lens = torch.full(size=(imgs.size(0),), fill_value=logits.size(0), dtype=torch.long).to(DEVICE)\n",
        "\n",
        "            loss = criterion(logits.log_softmax(2), targets, input_lens, target_lens)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_accum += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        count = 0\n",
        "        spot_true = \"\"\n",
        "        spot_pred = \"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, _, _, texts) in enumerate(val_loader):\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                logits = model(imgs)\n",
        "                preds = decode(logits)\n",
        "                for j, (pred, true_text) in enumerate(zip(preds, texts)):\n",
        "                    if pred == true_text: val_correct += 1\n",
        "                    count += 1\n",
        "                    if random.random() < 0.05:\n",
        "                        spot_true = true_text\n",
        "                        spot_pred = pred\n",
        "\n",
        "        accuracy = val_correct / count\n",
        "        print(f\"\\nüìä Epoch {epoch} - Loss: {loss_accum/len(train_loader):.4f} | Acc: {accuracy*100:.2f}%\")\n",
        "\n",
        "        print(f\"   üîç Random Sample: True: {spot_true} | Pred: {spot_pred}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "            print(f\"üî• Saved New Best Model! ({accuracy*100:.2f}%)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    resume_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIVRcLLid6PP",
        "outputId": "89459a9c-73d1-42f5-b53c-eeaf58bab9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Resuming on Device: cuda\n",
            "‚è≥ Loading Data...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Found checkpoint: /content/drive/MyDrive/pytorch_ocr_checkpoints/resnet_best.pth\n",
            "‚úÖ Weights Loaded! Continuing training...\n",
            "üöÄ Training Loop Restarted...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:50<00:00,  7.83it/s, loss=0.2368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 1 - Loss: 0.2730 | Acc: 34.66%\n",
            "   üîç Random Sample: True: ‡¥Æ‡¥≤‡¥ô‡µç‡¥ï‡¥∞ | Pred: ‡¥Æ‡¥∂‡µç‡¥¶‡¥∞\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:45<00:00,  8.06it/s, loss=0.0849]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 2 - Loss: 0.2436 | Acc: 35.23%\n",
            "   üîç Random Sample: True: ‡¥µ‡¥æ‡¥≤‡µç‡¥Ø‡µÅ‡¥µ‡¥ø‡¥®‡µç‡¥±‡µÜ | Pred: ‡¥µ‡¥≤‡µç‡¥™‡µÅ‡¥ø‡¥®‡µç‡¥±‡µÜ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:44<00:00,  8.09it/s, loss=0.1225]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 3 - Loss: 0.2211 | Acc: 37.45%\n",
            "   üîç Random Sample: True: ‡¥∏‡¥æ‡¥ô‡µç‡¥ï‡µá‡¥§‡¥ø‡¥ï‡¥µ‡¥ø‡¥¶‡µç‡¥Ø | Pred: ‡¥∏‡¥æ‡¥ô‡µç‡¥ï‡µá‡¥§‡¥ø‡¥µ‡¥ø‡¥¶‡µç‡¥Ø\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:44<00:00,  8.10it/s, loss=0.3013]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 4 - Loss: 0.1995 | Acc: 40.01%\n",
            "   üîç Random Sample: True: ‡¥™‡µÅ‡¥∞‡µã‡¥ó‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥µ‡µÜ | Pred: ‡¥™‡µÅ‡¥∞‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡¥µ‡µÜ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:42<00:00,  8.23it/s, loss=0.2663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 5 - Loss: 0.1850 | Acc: 42.61%\n",
            "   üîç Random Sample: True: ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø | Pred: ‡¥â‡¥π‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Æ‡¥æ‡¥ï‡¥ø\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:44<00:00,  8.10it/s, loss=0.1337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 6 - Loss: 0.1726 | Acc: 42.44%\n",
            "   üîç Random Sample: True: ‡¥∏‡¥Ç‡¥∞‡¥ï‡µç‡¥∑‡¥£ | Pred: ‡¥∏‡¥Ç‡¥∞‡¥ï‡µç‡¥∑‡¥£\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1333/1333 [02:47<00:00,  7.98it/s, loss=0.4351]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 7 - Loss: 0.1611 | Acc: 42.61%\n",
            "   üîç Random Sample: True: ‡¥™‡¥ø‡¥±‡¥µ‡¥Ç | Pred: ‡¥™‡¥ø‡¥±‡¥µ‡¥Ç\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resume Epoch 8:  40%|‚ñà‚ñà‚ñà‚ñâ      | 531/1333 [01:06<01:29,  8.98it/s, loss=0.1810]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHiq2wp6eHdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}